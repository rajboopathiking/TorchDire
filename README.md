# ğŸ”¬ QGFD: Diffusion-Regularized Attention Replacement

This repository provides a **universal, model-agnostic wrapper** that injects a
**QGFD (Quasi-Gaussian Feature Diffusion)** attention mechanism into *any*
PyTorch / HuggingFace transformer model â€” without modifying model internals.

The system includes:

- **`MultiHeadQGFDLayer`** â€” a drop-in replacement for multi-head attention  
- **`SafeWrappedAttention`** â€” a universal wrapper that replaces existing attention modules  
- **`wrap_model_with_qgfd(...)`** â€” recursively rewrites a full model in-place  
- **`QGFD_Sanity_Checks.py`** â€” automated shape, gradient, and wrapping tests

This library is intended for research experiments in stabilizing attention,
regularizing attention distributions via diffusion, and replacing dot-product
attention with a more structured update rule.

---

## âœ¨ Features

### âœ” Universal Attention Replacement
The wrapper detects any leaf module whose class name contains `"Attention"` and that
has parameters typical of Q/K/V projections. It then replaces it with
`SafeWrappedAttention`, preserving:

- original module behavior  
- caching (`present_key_value`)  
- attention masks  
- `output_attentions=True` compatibility  

### âœ” QGFD Diffusion Layer
`MultiHeadQGFDLayer` performs:

1. standard Q/K/V projection  
2. baseline softmax attention  
3. **repeated diffusion steps**:  
   \[
     p_{t+1} = (1 - \alpha)p_0 + \alpha(p_t P)
   \]  
   where `P` is a normalized *key-similarity transition matrix*

4. final value projection

Includes:
- cosine-sim transition matrix  
- temperature scaling  
- warmup schedule for Î±  
- early stopping  
- gradient-enabled diffusion (`detach_P=False` by default)  

### âœ” Safety and Stability
The wrapper includes:
- robust attribute copying  
- weight transfer from original attention  
- verification step ensuring proper `.qgfd` and `._orig` attachment  
- fallback heuristics for ambiguous architectures  

### âœ” Fully Tested
Run:

```bash
python QGFD_Sanity_Checks.py
```
This validates:

QGFD layer shapes & step counter

mask propagation

model wrapping on a tiny synthetic model

optional HF model wrapping smoke test

ğŸ“¦ Installation
Clone:

```bash
git clone https://github.com/YOURNAME/YOURREPO.git
cd YOURREPO
```
Install dependencies:

```bash
pip install torch transformers
```
(Transformers optional unless running HF smoke test)

ğŸš€ Usage
1. Import components
```python
from qgfd_attention import MultiHeadQGFDLayer
from universal_qgfd_replacer import wrap_model_with_qgfd
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
# Wrap all its attention layers
model = wrap_model_with_qgfd(
    model,
    MultiHeadQGFDLayer,
    diffusion_steps=4,
    target_alpha=0.02,
    warmup_steps=20000,
    detach_P=False,
    temp=1.0,
    verbose=True
)
# Run inference as usual
outputs = model(input_ids)

```
The model behaves identically interface-wise, but internally runs QGFD attention.

âš™ How It Works
SafeWrappedAttention
This meta-module:

stores original module in . _orig

instantiates a QGFD layer as .qgfd

copies original public attributes

intercepts the forward pass

maintains:

caches

masks

attention outputs

wrap_model_with_qgfd
It traverses all named submodules:

Detects attention blocks with is_leaf_attention

Instantiates a wrapper

Installs it using _set_submodule

Verifies correct replacement

Prints a summary

Supports:

ModuleList

nested attributes

tuple/list submodules

attention classes in encoder/decoder blocks

ğŸ§ª Running Sanity Tests
```bash
python QGFD_Sanity_Checks.py
```
This runs 4 checks:

Check	Description
1	QGFD layer forward shape / step counter
2	Attention mask correctness
3	Wrapping a tiny synthetic model
4	Optional HF model smoke test

âœ” Pass = everything structurally healthy.

ğŸ“ Repository Structure
pgsql
Copy code
â”œâ”€â”€ qgfd_attention.py             # QGFD attention implementation
â”œâ”€â”€ universal_qgfd_replacer.py    # Universal attention wrapper
â”œâ”€â”€ QGFD_Sanity_Checks.py         # Tests
â””â”€â”€ README.md                     # This file
ğŸ“œ License
MIT License â€” free for commercial and research use.

ğŸ¤ Contributing
Pull requests welcome â€” especially for:

supporting more transformer architectures

speedups (e.g., batched P approximations)

QGFD variants

integration with FlashAttention
